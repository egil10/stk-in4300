family=binomial, data=tr2, method="REML", select=TRUE)
te_gam2 <- mean(ifelse(predict(g2, te2, type="response")>0.5,"pos","neg") != yte2)
tree2 <- rpart(diabetes~., data=tr2, method="class", cp=0.001)
cp_opt2 <- tree2$cptable[which.min(tree2$cptable[,"xerror"]), "CP"]
tree2 <- prune(tree2, cp=cp_opt2)
te_tree2 <- mean(predict(tree2,te2,type="class")!=yte2)
bag2 <- bagging(diabetes~., data=tr2, nbagg=200)
te_bag2 <- mean(predict(bag2,te2,type="class")!=yte2)
rf2 <- randomForest(diabetes~., data=tr2, ntree=500)
te_rf2 <- mean(predict(rf2,te2)!=yte2)
cat("2e_knn", te_knn2, "2e_gam", te_gam2, "2e_tree", te_tree2, "2e_bag", te_bag2, "2e_rf", te_rf2, "\n")
library(tidyverse)
library(mlbench)
library(class)
library(mgcv)
library(rpart)
library(rpart.plot)
library(ipred)
library(randomForest)
set.seed(4300)
theme_set(theme_bw())
# problem 2
data("PimaIndiansDiabetes")
df <- as_tibble(PimaIndiansDiabetes)
# a)
ix_pos <- which(df$diabetes=="pos"); ix_neg <- which(df$diabetes=="neg")
tr_ix <- c(sample(ix_pos, round(2*length(ix_pos)/3)), sample(ix_neg, round(2*length(ix_neg)/3)))
tr <- df[tr_ix,]; te <- df[-tr_ix,]
Xtr <- scale(tr %>% dplyr::select(-diabetes))
ctr <- attr(Xtr,"scaled:center"); str <- attr(Xtr,"scaled:scale")
Xte <- scale(te %>% dplyr::select(-diabetes), center=ctr, scale=str)
ytr <- tr$diabetes; yte <- te$diabetes
kvals <- 1:30
cv5 <- rep(NA, length(kvals))
loocv <- rep(NA, length(kvals))
te_err <- rep(NA, length(kvals))
folds <- 5
pos_f <- split(sample(which(ytr=="pos")), rep(1:folds, length.out=sum(ytr=="pos")))
neg_f <- split(sample(which(ytr=="neg")), rep(1:folds, length.out=sum(ytr=="neg")))
for(i in seq_along(kvals)){
k <- kvals[i]
cv5[i] <- mean(sapply(1:folds, function(f){
tr_id <- c(unlist(pos_f[-f]), unlist(neg_f[-f]))
va_id <- c(unlist(pos_f[f]),  unlist(neg_f[f]))
pr <- knn(train=Xtr[tr_id,], test=Xtr[va_id,], cl=ytr[tr_id], k=k)
mean(pr != ytr[va_id])
}))
loocv[i] <- mean(knn.cv(Xtr, ytr, k=k) != ytr)
te_err[i] <- mean(knn(train=Xtr, test=Xte, cl=ytr, k=k) != yte)
}
p <- tibble(k=kvals, cv5=cv5, loocv=loocv, test=te_err) %>%
pivot_longer(-k, names_to="type", values_to="err") %>%
ggplot(aes(k,err,linetype=type)) + geom_line()
ggsave("plots/plot2a.pdf", p, width=10, height=6)
cat("2a_cv5_min", min(cv5), "2a_loocv_min", min(loocv),
"2a_test_at_k_cv5", te_err[which.min(cv5)], "\n")
# b)
g_full <- gam(
diabetes ~ s(pregnant)+s(glucose)+s(pressure)+s(triceps)+s(insulin)+s(mass)+s(pedigree)+s(age),
family=binomial, data=tr, method="REML", select=TRUE)
pr_b <- ifelse(predict(g_full, te, type="response")>0.5,"pos","neg")
te_gam <- mean(pr_b != yte)
cat("2b_gam_test", te_gam, "\n")
# c)
tree <- rpart(diabetes~., data=tr, method="class", cp=0.001)
cp_opt <- tree$cptable[which.min(tree$cptable[,"xerror"]), "CP"]
tree <- prune(tree, cp=cp_opt)
p_tr <- predict(tree, tr, type="class"); p_te <- predict(tree, te, type="class")
tr_tree <- mean(p_tr!=ytr); te_tree <- mean(p_te!=yte)
pdf("plots/plot2c_tree.pdf", width=10, height=6); rpart.plot(tree); dev.off()
bag <- bagging(diabetes~., data=tr, nbagg=200)
tr_bag <- mean(predict(bag,tr,type="class")!=ytr)
te_bag <- mean(predict(bag,te,type="class")!=yte)
rf <- randomForest(diabetes~., data=tr, ntree=500)
tr_rf <- mean(predict(rf,tr)!=ytr)
te_rf <- mean(predict(rf,te)!=yte)
cat("2c_tree", tr_tree, te_tree, "2c_bag", tr_bag, te_bag, "2c_rf", tr_rf, te_rf, "\n")
# d)
cat("2d_best_by_test",
c("tree"=te_tree,"bag"=te_bag,"rf"=te_rf,"gam"=te_gam)[
which.min(c(te_tree,te_bag,te_rf,te_gam))
], "\n")
# e)
data("PimaIndiansDiabetes2")
df2 <- as_tibble(PimaIndiansDiabetes2) %>% drop_na()
ix_pos2 <- which(df2$diabetes=="pos"); ix_neg2 <- which(df2$diabetes=="neg")
tr_ix2 <- c(sample(ix_pos2, round(2*length(ix_pos2)/3)), sample(ix_neg2, round(2*length(ix_neg2)/3)))
tr2 <- df2[tr_ix2,]; te2 <- df2[-tr_ix2,]
Xtr2 <- scale(tr2 %>% dplyr::select(-diabetes))
ctr2 <- attr(Xtr2,"scaled:center"); str2 <- attr(Xtr2,"scaled:scale")
Xte2 <- scale(te2 %>% dplyr::select(-diabetes), center=ctr2, scale=str2)
ytr2 <- tr2$diabetes; yte2 <- te2$diabetes
k <- kvals[which.min(cv5)]
te_knn2 <- mean(knn(train=Xtr2, test=Xte2, cl=ytr2, k=k) != yte2)
g2 <- gam(
diabetes ~ s(pregnant)+s(glucose)+s(pressure)+s(triceps)+s(insulin)+s(mass)+s(pedigree)+s(age),
family=binomial, data=tr2, method="REML", select=TRUE)
te_gam2 <- mean(ifelse(predict(g2, te2, type="response")>0.5,"pos","neg") != yte2)
tree2 <- rpart(diabetes~., data=tr2, method="class", cp=0.001)
cp_opt2 <- tree2$cptable[which.min(tree2$cptable[,"xerror"]), "CP"]
tree2 <- prune(tree2, cp=cp_opt2)
te_tree2 <- mean(predict(tree2,te2,type="class")!=yte2)
bag2 <- bagging(diabetes~., data=tr2, nbagg=200)
te_bag2 <- mean(predict(bag2,te2,type="class")!=yte2)
rf2 <- randomForest(diabetes~., data=tr2, ntree=500)
te_rf2 <- mean(predict(rf2,te2)!=yte2)
cat("2e_knn", te_knn2, "2e_gam", te_gam2, "2e_tree", te_tree2, "2e_bag", te_bag2, "2e_rf", te_rf2, "\n")
library(tidyverse)
library(MASS)
library(rpart)
library(rpart.plot)
library(glmnet)
library(mgcv)
set.seed(4300)
theme_set(theme_bw())
# ========================================================
# Problem 1: QSAR Aquatic Toxicity Regression
# ========================================================
# --------------------------------------------------------
# Load and prepare data
# --------------------------------------------------------
df <- read.csv("data/qsar_aquatic_toxicity.csv", sep=";", header=FALSE) %>%
as_tibble()
colnames(df) <- c("TPSA","SAacc","H050","MLOGP","RDCHI","GATS1p","nN","C040","LC50")
# Train/test split (≈ 2/3 training, 1/3 test)
i  <- sample(1:nrow(df), round(2 * nrow(df) / 3))
tr <- df[i, ]
te <- df[-i, ]
# --------------------------------------------------------
# (a) Linear models: raw counts vs. dummy-encoded counts
# --------------------------------------------------------
# Model with raw counts included as linear terms
m1 <- lm(LC50 ~ ., data = tr)
# Dummy model: use 0/1 indicators for count variables
m2 <- lm(LC50 ~ TPSA + SAacc + MLOGP + RDCHI + GATS1p +
I(H050 > 0) + I(nN > 0) + I(C040 > 0),
data = tr)
# Predictions and train/test MSE
p1 <- predict(m1, tr);  q1 <- predict(m1, te)
p2 <- predict(m2, tr);  q2 <- predict(m2, te)
cat("train_mse_m1", mean((tr$LC50 - p1)^2),
"test_mse_m1",  mean((te$LC50 - q1)^2), "\n")
cat("train_mse_m2", mean((tr$LC50 - p2)^2),
"test_mse_m2",  mean((te$LC50 - q2)^2), "\n")
# Scatterplot of observed vs predicted
p <- ggplot(tibble(obs = te$LC50, pred = q1),
aes(obs, pred)) +
geom_point() +
geom_abline()
ggsave("plots/plot1a.pdf", p, width=10, height=6)
# --------------------------------------------------------
# (b) Repeat experiment 200 times to estimate test error distribution
# --------------------------------------------------------
e1 <- numeric(200)  # raw counts model
e2 <- numeric(200)  # dummy model
for (k in 1:200) {
i  <- sample(1:nrow(df), round(2 * nrow(df) / 3))
tr <- df[i, ]; te <- df[-i, ]
m1 <- lm(LC50 ~ ., data=tr)
m2 <- lm(LC50 ~ TPSA + SAacc + MLOGP + RDCHI + GATS1p +
I(H050 > 0) + I(nN > 0) + I(C040 > 0),
data=tr)
e1[k] <- mean((te$LC50 - predict(m1, te))^2)
e2[k] <- mean((te$LC50 - predict(m2, te))^2)
}
# Boxplot of empirical MSE distributions
t <- tibble(model = rep(c("raw","dummy"), each=200),
mse   = c(e1, e2))
p <- ggplot(t, aes(model, mse)) + geom_boxplot()
ggsave("plots/plot1b.pdf", p, width=10, height=6)
cat("mean_test_mse_raw", mean(e1), "\n")
cat("mean_test_mse_dummy", mean(e2), "\n")
# --------------------------------------------------------
# (c) Variable selection: forward/backward with AIC and BIC
# --------------------------------------------------------
full <- lm(LC50 ~ ., data=tr)
null <- lm(LC50 ~ 1, data=tr)
b_aic <- step(full, direction="backward", k=2, trace=0)
f_aic <- step(null, scope=formula(full), direction="forward", k=2, trace=0)
b_bic <- step(full, direction="backward", k=log(nrow(tr)), trace=0)
f_bic <- step(null, scope=formula(full), direction="forward", k=log(nrow(tr)), trace=0)
cat("backward_AIC:",  deparse(formula(b_aic)), "\n")
cat("forward_AIC :",  deparse(formula(f_aic)), "\n")
cat("backward_BIC:", deparse(formula(b_bic)), "\n")
cat("forward_BIC :", deparse(formula(f_bic)), "\n")
# --------------------------------------------------------
# (d) Ridge regression: CV vs bootstrap OOB estimate
# --------------------------------------------------------
xtr <- as.matrix(select(tr, -LC50)); ytr <- tr$LC50
library(tidyverse)
library(MASS)
library(rpart)
library(rpart.plot)
library(glmnet)
library(mgcv)
set.seed(4300)
theme_set(theme_bw())
# ========================================================
# Problem 1: QSAR Aquatic Toxicity Regression
# ========================================================
# --------------------------------------------------------
# Load and prepare data
# --------------------------------------------------------
df <- read.csv("data/qsar_aquatic_toxicity.csv", sep=";", header=FALSE) %>%
as_tibble()
colnames(df) <- c("TPSA","SAacc","H050","MLOGP","RDCHI","GATS1p","nN","C040","LC50")
# Train/test split (≈ 2/3 training, 1/3 test)
i  <- sample(1:nrow(df), round(2 * nrow(df) / 3))
tr <- df[i, ]
te <- df[-i, ]
# --------------------------------------------------------
# (a) Linear models: raw counts vs. dummy-encoded counts
# --------------------------------------------------------
# Model with raw counts included as linear terms
m1 <- lm(LC50 ~ ., data = tr)
# Dummy model: use 0/1 indicators for count variables
m2 <- lm(LC50 ~ TPSA + SAacc + MLOGP + RDCHI + GATS1p +
I(H050 > 0) + I(nN > 0) + I(C040 > 0),
data = tr)
# Predictions and train/test MSE
p1 <- predict(m1, tr);  q1 <- predict(m1, te)
p2 <- predict(m2, tr);  q2 <- predict(m2, te)
cat("train_mse_m1", mean((tr$LC50 - p1)^2),
"test_mse_m1",  mean((te$LC50 - q1)^2), "\n")
cat("train_mse_m2", mean((tr$LC50 - p2)^2),
"test_mse_m2",  mean((te$LC50 - q2)^2), "\n")
# Scatterplot of observed vs predicted
p <- ggplot(tibble(obs = te$LC50, pred = q1),
aes(obs, pred)) +
geom_point() +
geom_abline()
ggsave("plots/plot1a.pdf", p, width=10, height=6)
# --------------------------------------------------------
# (b) Repeat experiment 200 times to estimate test error distribution
# --------------------------------------------------------
e1 <- numeric(200)  # raw counts model
e2 <- numeric(200)  # dummy model
for (k in 1:200) {
i  <- sample(1:nrow(df), round(2 * nrow(df) / 3))
tr <- df[i, ]; te <- df[-i, ]
m1 <- lm(LC50 ~ ., data=tr)
m2 <- lm(LC50 ~ TPSA + SAacc + MLOGP + RDCHI + GATS1p +
I(H050 > 0) + I(nN > 0) + I(C040 > 0),
data=tr)
e1[k] <- mean((te$LC50 - predict(m1, te))^2)
e2[k] <- mean((te$LC50 - predict(m2, te))^2)
}
# Boxplot of empirical MSE distributions
t <- tibble(model = rep(c("raw","dummy"), each=200),
mse   = c(e1, e2))
p <- ggplot(t, aes(model, mse)) + geom_boxplot()
ggsave("plots/plot1b.pdf", p, width=10, height=6)
cat("mean_test_mse_raw", mean(e1), "\n")
cat("mean_test_mse_dummy", mean(e2), "\n")
# --------------------------------------------------------
# (c) Variable selection: forward/backward with AIC and BIC
# --------------------------------------------------------
full <- lm(LC50 ~ ., data=tr)
null <- lm(LC50 ~ 1, data=tr)
b_aic <- step(full, direction="backward", k=2, trace=0)
f_aic <- step(null, scope=formula(full), direction="forward", k=2, trace=0)
b_bic <- step(full, direction="backward", k=log(nrow(tr)), trace=0)
f_bic <- step(null, scope=formula(full), direction="forward", k=log(nrow(tr)), trace=0)
cat("backward_AIC:",  deparse(formula(b_aic)), "\n")
cat("forward_AIC :",  deparse(formula(f_aic)), "\n")
cat("backward_BIC:", deparse(formula(b_bic)), "\n")
cat("forward_BIC :", deparse(formula(f_bic)), "\n")
# --------------------------------------------------------
# (d) Ridge regression: CV vs bootstrap OOB estimate
# --------------------------------------------------------
xtr <- as.matrix(dplyr::select(tr, -LC50))
ytr <- tr$LC50
xte <- as.matrix(dplyr::select(te, -LC50))
yte <- te$LC50
lambda_grid <- 10^seq(-4, 4, length=100)
# Cross-validation
cv <- cv.glmnet(xtr, ytr, alpha=0, lambda=lambda_grid, standardize=TRUE)
lam_cv <- cv$lambda.min
tr_cv <- mean((ytr - predict(cv$glmnet.fit, s=lam_cv, newx=xtr))^2)
te_cv <- mean((yte - predict(cv$glmnet.fit, s=lam_cv, newx=xte))^2)
# Bootstrap OOB
B <- 200
oob_mse <- matrix(NA, B, length(lambda_grid))
for(b in 1:B){
idx <- sample(seq_len(nrow(tr)), replace=TRUE)
oob <- setdiff(seq_len(nrow(tr)), unique(idx))
fit <- glmnet(xtr[idx,], ytr[idx], alpha=0, lambda=lambda_grid, standardize=TRUE)
if(length(oob) > 5){
pred_oob <- predict(fit, newx=xtr[oob,])
oob_mse[b,] <- colMeans((ytr[oob] - pred_oob)^2)
}
}
boot_curve <- colMeans(oob_mse, na.rm=TRUE)
lam_boot <- lambda_grid[which.min(boot_curve)]
# Save CV vs bootstrap plot
p <- tibble(log_lambda=log(lambda_grid),
cv=cv$cvm[match(lambda_grid, cv$lambda)],
boot=boot_curve) %>%
pivot_longer(-log_lambda, names_to="method", values_to="mse") %>%
ggplot(aes(log_lambda, mse, linetype=method)) +
geom_line()
ggsave("plots/plot1d.pdf", p, width=10, height=6)
# --------------------------------------------------------
# (e) GAM models with different smoothness
# --------------------------------------------------------
g1 <- gam(LC50 ~ s(TPSA,k=4) + s(SAacc,k=4) + s(H050,k=3) +
s(MLOGP,k=4) + s(RDCHI,k=4) + s(GATS1p,k=4) +
s(nN,k=3) + s(C040,k=3),
data=tr, method="REML")
g2 <- gam(LC50 ~ s(TPSA,k=7) + s(SAacc,k=7) + s(H050,k=3) +
s(MLOGP,k=7) + s(RDCHI,k=7) + s(GATS1p,k=7) +
s(nN,k=3) + s(C040,k=3),
data=tr, method="REML")
tr_g1 <- mean((tr$LC50 - predict(g1,tr))^2)
te_g1 <- mean((te$LC50 - predict(g1,te))^2)
tr_g2 <- mean((tr$LC50 - predict(g2,tr))^2)
te_g2 <- mean((te$LC50 - predict(g2,te))^2)
# --------------------------------------------------------
# (f) Regression tree + cost-complexity pruning
# --------------------------------------------------------
tree0 <- rpart(LC50 ~ ., data=tr, method="anova", cp=0.001)
cp_opt <- tree0$cptable[which.min(tree0$cptable[,"xerror"]), "CP"]
tree <- prune(tree0, cp=cp_opt)
tr_tree <- mean((tr$LC50 - predict(tree,tr))^2)
te_tree <- mean((te$LC50 - predict(tree,te))^2)
pdf("plots/plot1f_tree.pdf", width=10, height=6); rpart.plot(tree); dev.off()
pdf("plots/plot1f_cp.pdf",   width=10, height=6); plotcp(tree0); dev.off()
# --------------------------------------------------------
# (g) Compare all models
# --------------------------------------------------------
res <- tibble(
model=c("lm_raw","lm_dummy","step_back_AIC","step_fwd_AIC",
"step_back_BIC","step_fwd_BIC","ridge_cv","ridge_boot",
"gam_k4","gam_k7","tree"),
train=c(mean((tr$LC50 - p1)^2), mean((tr$LC50 - p2)^2),
mean((tr$LC50 - predict(b_aic,tr))^2),
mean((tr$LC50 - predict(f_aic,tr))^2),
mean((tr$LC50 - predict(b_bic,tr))^2),
mean((tr$LC50 - predict(f_bic,tr))^2),
tr_cv, tr_boot, tr_g1, tr_g2, tr_tree),
test=c(mean((te$LC50 - q1)^2), mean((te$LC50 - q2)^2),
mean((te$LC50 - predict(b_aic,te))^2),
mean((te$LC50 - predict(f_aic,te))^2),
mean((te$LC50 - predict(b_bic,te))^2),
mean((te$LC50 - predict(f_bic,te))^2),
te_cv, te_boot, te_g1, te_g2, te_tree)
) %>% arrange(test)
print(res)
library(tidyverse)
library(mlbench)
library(class)
library(mgcv)
library(rpart)
library(rpart.plot)
library(ipred)
library(randomForest)
set.seed(4300)
theme_set(theme_bw())
# ------------------------------------------------------------
# Problem 2 — Classification on PimaIndiansDiabetes
# ------------------------------------------------------------
data("PimaIndiansDiabetes")
df <- as_tibble(PimaIndiansDiabetes)
# ---------------------------
# (a) Train/test split + kNN
# ---------------------------
# Stratified split: keep class balance
ix_pos <- which(df$diabetes == "pos")
ix_neg <- which(df$diabetes == "neg")
tr_ix <- c(
sample(ix_pos, round(2 * length(ix_pos) / 3)),
sample(ix_neg, round(2 * length(ix_neg) / 3))
)
tr <- df[tr_ix, ]
te <- df[-tr_ix, ]
# Standardize predictors using training mean/sd
Xtr <- scale(dplyr::select(tr, -diabetes))
ctr <- attr(Xtr, "scaled:center")
str <- attr(Xtr, "scaled:scale")
Xte <- scale(dplyr::select(te, -diabetes), center = ctr, scale = str)
ytr <- tr$diabetes
yte <- te$diabetes
kvals <- 1:30
cv5   <- rep(NA, length(kvals))
loocv <- rep(NA, length(kvals))
te_err <- rep(NA, length(kvals))
# Stratified 5-fold CV
folds <- 5
pos_f <- split(sample(which(ytr == "pos")), rep(1:folds, length.out = sum(ytr == "pos")))
neg_f <- split(sample(which(ytr == "neg")), rep(1:folds, length.out = sum(ytr == "neg")))
for(i in seq_along(kvals)){
k <- kvals[i]
# 5-fold CV error
cv5[i] <- mean(sapply(1:folds, function(f){
tr_id <- c(unlist(pos_f[-f]), unlist(neg_f[-f]))
va_id <- c(unlist(pos_f[f]),  unlist(neg_f[f]))
pr <- knn(train = Xtr[tr_id,], test = Xtr[va_id,], cl = ytr[tr_id], k = k)
mean(pr != ytr[va_id])
}))
# LOOCV error
loocv[i] <- mean(knn.cv(Xtr, ytr, k = k) != ytr)
# Test error
te_err[i] <- mean(knn(train = Xtr, test = Xte, cl = ytr, k = k) != yte)
}
# Plot all three error curves
p <- tibble(k = kvals, cv5 = cv5, loocv = loocv, test = te_err) %>%
pivot_longer(-k, names_to = "type", values_to = "err") %>%
ggplot(aes(k, err, linetype = type)) + geom_line()
ggsave("plots/plot2a.pdf", p, width = 10, height = 6)
cat("2a_cv5_min", min(cv5),
"2a_loocv_min", min(loocv),
"2a_test_at_k_cv5", te_err[which.min(cv5)], "\n")
# ---------------------------
# (b) GAM model
# ---------------------------
g_full <- gam(
diabetes ~ s(pregnant) + s(glucose) + s(pressure) + s(triceps) +
s(insulin) + s(mass) + s(pedigree) + s(age),
family = binomial,
data = tr,
method = "REML",
select = TRUE
)
# Classify based on predicted probability > 0.5
pr_b <- ifelse(predict(g_full, te, type = "response") > 0.5, "pos", "neg")
te_gam <- mean(pr_b != yte)
cat("2b_gam_test", te_gam, "\n")
# ---------------------------
# (c) Trees, Bagging, Random Forest
# ---------------------------
# CART tree + prune using optimal CP
tree <- rpart(diabetes ~ ., data = tr, method = "class", cp = 0.001)
cp_opt <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
tree <- prune(tree, cp = cp_opt)
p_tr <- predict(tree, tr, type = "class")
p_te <- predict(tree, te, type = "class")
tr_tree <- mean(p_tr != ytr)
te_tree <- mean(p_te != yte)
# Save tree plot
pdf("plots/plot2c_tree.pdf", width = 10, height = 6)
rpart.plot(tree)
dev.off()
# Bagging
bag <- bagging(diabetes ~ ., data = tr, nbagg = 200)
tr_bag <- mean(predict(bag, tr, type = "class") != ytr)
te_bag <- mean(predict(bag, te, type = "class") != yte)
# Random Forest
rf <- randomForest(diabetes ~ ., data = tr, ntree = 500)
tr_rf <- mean(predict(rf, tr) != ytr)
te_rf <- mean(predict(rf, te) != yte)
cat("2c_tree", tr_tree, te_tree,
"2c_bag", tr_bag, te_bag,
"2c_rf", tr_rf, te_rf, "\n")
# ---------------------------
# (d) Best model by test error
# ---------------------------
cat("2d_best_by_test",
c("tree" = te_tree,
"bag" = te_bag,
"rf"  = te_rf,
"gam" = te_gam)[ which.min(c(te_tree, te_bag, te_rf, te_gam)) ],
"\n")
# ---------------------------
# (e) Repeat analysis on cleaned dataset
# ---------------------------
data("PimaIndiansDiabetes2")
df2 <- as_tibble(PimaIndiansDiabetes2) %>% drop_na()
# Stratified split again
ix_pos2 <- which(df2$diabetes == "pos")
ix_neg2 <- which(df2$diabetes == "neg")
tr_ix2 <- c(
sample(ix_pos2, round(2 * length(ix_pos2) / 3)),
sample(ix_neg2, round(2 * length(ix_neg2) / 3))
)
tr2 <- df2[tr_ix2, ]
te2 <- df2[-tr_ix2, ]
# Standardize
Xtr2 <- scale(dplyr::select(tr2, -diabetes))
ctr2 <- attr(Xtr2, "scaled:center")
str2 <- attr(Xtr2, "scaled:scale")
Xte2 <- scale(dplyr::select(te2, -diabetes), center = ctr2, scale = str2)
ytr2 <- tr2$diabetes
yte2 <- te2$diabetes
# Use best k from (a)
k <- kvals[which.min(cv5)]
te_knn2 <- mean(knn(train = Xtr2, test = Xte2, cl = ytr2, k = k) != yte2)
# GAM on cleaned data
g2 <- gam(
diabetes ~ s(pregnant) + s(glucose) + s(pressure) + s(triceps) +
s(insulin) + s(mass) + s(pedigree) + s(age),
family = binomial,
data = tr2,
method = "REML",
select = TRUE
)
te_gam2 <- mean(ifelse(predict(g2, te2, type = "response") > 0.5, "pos", "neg") != yte2)
# Tree
tree2 <- rpart(diabetes ~ ., data = tr2, method = "class", cp = 0.001)
cp_opt2 <- tree2$cptable[which.min(tree2$cptable[, "xerror"]), "CP"]
tree2 <- prune(tree2, cp = cp_opt2)
te_tree2 <- mean(predict(tree2, te2, type = "class") != yte2)
# Bagging
bag2 <- bagging(diabetes ~ ., data = tr2, nbagg = 200)
te_bag2 <- mean(predict(bag2, te2, type = "class") != yte2)
# Random Forest
rf2 <- randomForest(diabetes ~ ., data = tr2, ntree = 500)
te_rf2 <- mean(predict(rf2, te2) != yte2)
cat("2e_knn", te_knn2,
"2e_gam", te_gam2,
"2e_tree", te_tree2,
"2e_bag", te_bag2,
"2e_rf", te_rf2, "\n")
